model: hilcodec
model_kwargs:
    channels_enc: 64
    channels_dec: 96
    n_fft_base: 64
    n_residual_enc: 2      # Number of residual blocks in the encoder. Since there exists SpecBlock, $N_{enc}=N_{resblock}+1$.
    n_residual_dec: 3
    res_scale_enc: 0.5773502691896258
    res_scale_dec: 0.5773502691896258
    strides: [8, 5, 4, 2]
    kernel_size: 5          # Kernel size of the first convolutional layer in the encoder.
    last_kernel_size: 5     # Kernel size of the last convolutional layer in the decoder.
    residual_kernel_size: 5 # Kernel size of the residual blocks.
    dilation_base: 1        # Base dilation factor of the residual blocks.
    skip: identity
    final_activation: Tanh
    act_all: False
    encoder_l2norm: True
    causal: True
    zero_init: True
    inout_norm: True
    pad_mode: constant      # padding mode for every (causal) convolutions.

    # SpecBlock parameters
    spec: stft
    spec_layer: 1x1_zero
    spec_compression: log
    spec_learnable: False   # Whether to learn STFT filters.

    vq_kwargs:
        dim: 128                # dimension (channel size) of each vector
        codebook_size: 1024     # 0.75kbps/codebook
        num_quantizers: 12      # Total number of codebooks in residual VQ
        kmeans_init: True
        decay: 0.99             # ema decay factor
        ema_num_threshold: 0.5  # when #usage of a vector < threshold, re-initialize it.
        ema_num_initial: 0.5    # When re-initialized, the initial #usage is set to this value.
        dropout: True
        dropout_index: [2, 4, 8, 12]  # 1.5, 3, 6, 9 kbps
disc_kwargs:
    mfbd_kwargs:
        use: True
        channels: [32, 128, 512, 1024, 1024]
        kernel_sizes: [5, 5, 5, 5, 5]
        strides: [3, 3, 3, 3, 1]
    mpd_kwargs:
        use: False
    msd_kwargs:
        use: False
    mstftd_kwargs:
        use: True
        magnitude: False
        n_ffts: [128, 256, 512, 1024]
        hop_lengths: [32, 64, 128, 256]
        win_lengths: [128, 256, 512, 1024]
        filters: 16         # Channel size of the first conv.
        filters_scale: 2    # Channel size scale factor for each conv.
data:
    dataset: 
        train: DirectoriesDataset   # load data from specified directories (utils/data/directories.py)
        valid: Dataset              # load data listed in a filelist (utils/data/audio.py)
        infer: Dataset
        pesq: Dataset
    classes:    # Used for DirectoriesDataset. Specify which directories to include and exclude.
        clean:
            directories_to_include:
                - "/home/shahn/Datasets/DNS-Challenge4/datasets_24khz/clean"
                - "/home/shahn/Datasets/VCTK-0.92/wav24_silence_trimmed"
            directories_to_exclude:
                - "/home/shahn/Datasets/VCTK-0.92/wav24_silence_trimmed/p225"
                - "/home/shahn/Datasets/VCTK-0.92/wav24_silence_trimmed/p226"
                - "/home/shahn/Datasets/VCTK-0.92/wav24_silence_trimmed/p227"
                - "/home/shahn/Datasets/VCTK-0.92/wav24_silence_trimmed/p228"
            extension: ".wav"
            probability: 0.67
            mix:
                noise: 0.5      # mix with a `noise` class with a probability of 0.5
        noise:
            directories_to_include:
                - "/home/shahn/Datasets/DNS-Challenge4/datasets_24khz/noise"
            extension: ".wav"
            probability: 0.0
        music:
            directories_to_include:
                - "/home/shahn/Datasets/jamendo/raw_30s_24khz"
            directories_to_exclude:
                - "/home/shahn/Datasets/jamendo/raw_30s_24kHz/99"
            extension: ".wav"
            probability: 0.33
    transforms:
        RandomGain:
            low_db: -10
            high_db: 6
    length: 150000      # Used for DirectoriesDataset. __len__() of a dataset (in samples).
    wav_dir: "/home/shahn/Datasets"
    data_dir: ""
    extension: ""
    num_infer: 6        # Number of samples to infer & plot in tensorboard
    filelists:
        valid: "filelists/DNS_VCTK_jamendo_valid_24khz.txt"
        infer: "filelists/infer_24khz.txt"
        pesq: "filelists/DNS_VCTK_jamendo_pesq_24khz.txt"
    filter:     # If set to True, data will be sorted by length.
        train: False
        valid: False
        pesq: True
    segment_size: 24000         # Segment size of each audio file for training & validation
    sampling_rate: 24000
    normalize_method: "random_gain"
    random_gain_low: 0.316      # -10 dB
    random_gain_high: 2.0       # +6 dB
    channels: 1                 # Audio channels

    # STFT parameters for tensorboard plotting
    n_fft: 1024
    hop_size: 256
    win_size: 1024
    clip_val: 1.0e-5            # clip value for log(-mel) spectrogram
train:
    batch_size: 24              # batch size for each GPU
    max_epochs: 50              # max epochs to train
    fp16_g: True
    fp16_d: True
    num_workers: 2              # num_workers for DataLoader
    persistent_workers: True    # set True when using train.py & num_workers > 0
    clip_grad: null
    seed: 1
    plot_param_and_grad: True   # plot histgram of params & grad in tensorboard
    save_interval: 10           # Save interval in epochs
    lookahead: 0
    infer_n: 4                  # 'n' of ResidualVQ for infer & pesq. 4 = 3kbps
    
    n_mels_max: 128
    mel_norm: null              # Normalization at mel filterbank. 'slaney' or null
    no_zero_at_mel_filter: True
    use_lsgan: False
    use_normalized_fm_loss: True

    balancer_kwargs:
        weights:
            ### rec:g:fm ~= 0.9:4:4 ###
            freq: 0.48          # reconstruction loss (Multi-Res MelSpec Loss)
            mfbd_g: 1.1
            mfbd_fm: 1.1
            mstftd_g: 1.1
            mstftd_fm: 1.1
        weight_others: 0.01
        ema_decay: 0.99

    optimizer: "AdamP"
    optimizer_kwargs:
        lr: 5.0e-4
        betas: [0.5, 0.9]
        weight_decay: 1.0e-5
    # optimizer_groups:
    #   - regex_list: ["weight_v"]    # regular expressions of parameter names
    #     project_channel: True       # optimizer_kwargs to update
    scheduler: CosineAnnealingWarmup
    scheduler_kwargs:
        warmup_iterations: 5000
        eta_min: 1.0e-6
        warn: False
valid:
    batch_size: 48
infer:
    interval: 10    # interval to infer in epoch
    batch_size: 1
pesq:
    # In our experiments, calculating ViSQOL during training makes the server crash,
    # so we turn it off.
    interval: 1000
    batch_size: 12
    num_workers: 0              # num_workers for a dataloader
    num_workers_executor: 12    # num_workers for a multiprocess executor
    metrics_to_calculate:
        pesq: False
        stoi: False
        visqol: False
        visqol_audio: False